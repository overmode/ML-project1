{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Generate the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "y_tr, input_data_tr, ids_tr = load_csv_data(\"./train.csv\")\n",
    "y_te, input_data_te, ids_te = load_csv_data(\"./test.csv\")\n",
    "input_data_play = input_data_tr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_csv_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print min and max of each field\n",
    "for collumn in input_data.T:\n",
    "    filtered_collumn = list(filter(lambda x:x !=-999, collumn))\n",
    "    print(min(filtered_collumn), max(filtered_collumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print distribution of valid values\n",
    "for i in range(30):\n",
    "    print(i)\n",
    "    input_cleaned = list(filter(lambda x : x != -999, input_data.T[i]))\n",
    "\n",
    "    n, bins, patches = plt.hist(input_cleaned, 500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find percentage of most frequent non-zero item\n",
    "from collections import Counter\n",
    "\n",
    "def Most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(1)[0][1]\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    input_cleaned = list(filter(lambda x : x!=-999 and x != 0, input_data.T[i]))\n",
    "    print (Most_Common(input_cleaned)/len(input_data.T[i])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find mapping between fields 22 and 29\n",
    "nuls = input_data[:,[22, 29]]\n",
    "for i in {0,1,2,3}:\n",
    "    input_cleaned = list(map(lambda x:x[1], filter(lambda x : x[0]==i, nuls)))\n",
    "    print(i, \"-> [\",min(input_cleaned), \";\", max(input_cleaned),\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print the graph of any given two fields\n",
    "import matplotlib.pyplot as plt\n",
    "tr = input_data.T\n",
    "to_test_1 = 28\n",
    "to_test_2 = 29\n",
    "a,b = zip(*filter(lambda x : x[0]!=-999 and x[1]!=-999, zip(tr[to_test_1], tr[to_test_2])))\n",
    "plt.plot(a, b, 'ro')\n",
    "plt.show()\n",
    "    \n",
    "#compare one field to every others\n",
    "#for i in range(30):\n",
    "#    print(i)\n",
    "#    a,b = zip(*filter(lambda x : x[0]!=-999 and x[1]!=-999, zip(tr[to_test], tr[i])))\n",
    "#    plt.plot(a, b, 'ro')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display correlation matrix\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(data=input_data)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used to research if one invalid item is also invalid for other fields\n",
    "inval = input_data[:,[0, 23, 24, 25,4,5,6,12,26,27,28]]\n",
    "are_inval_1 = -999\n",
    "are_inval_2 = -999 * 3\n",
    "are_inval_3 = -999 * 7\n",
    "full_inval_nb = len(list(filter((lambda x: ((x[0]!=are_inval_1) and sum(x[1:4]) ==are_inval_2 and (sum(x[4:])!=are_inval_3))), inval)))\n",
    "print(full_inval_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Least squares methods\n",
    "## 2.1 Least squares  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    w = np.pinv(tx) @ y\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 least squares with gradient descent\n",
    "### 2.2.1 Function implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma) :\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = -1\n",
    "    for n_iter in range(max_iters):\n",
    "        if (n_iter == max_iters - 1) :\n",
    "            loss = compute_loss_mse(y,tx,w)\n",
    "        \n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        w = w - gamma*grad           \n",
    "        \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 helper funtions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):   \n",
    "    e = y - (tx @ w)\n",
    "    return ((-1/len(y)) * (tx.T @ e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Least squares with stochastic gradient descent\n",
    "### 2.3.1 Function implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma) :\n",
    "    # Define parameters to store w and loss\n",
    "    \n",
    "    w = initial_w\n",
    "\n",
    "    loss = 0\n",
    "    batch_size = 1\n",
    "    \n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, max_iters):\n",
    "        loss = compute_loss_mse(y,tx,w)\n",
    "        grad = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        w = w - gamma*grad\n",
    "\n",
    "           \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    return compute_gradient(y,tx,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of batch_size matching elements from y and tx.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3 Ridge regression\n",
    "## 3.1 function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ): \n",
    "    \n",
    "    x_tr = tx.transpose()\n",
    "    lambda_prime = lambda_ * 2 * tx.shape[0]\n",
    "    X = (x_tr @ tx) + (lambda_prime * np.eye(tx.shape[1]))\n",
    "    Y = x_tr @ y\n",
    "    w_ridge = (np.linalg.solve(X, Y))    \n",
    "    loss = compute_loss_ridge(y, tx, w, lambda_)\n",
    "    return w_ridge, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "import numpy as np\n",
    "\n",
    "def compute_loss_ridge (y, tx, w, lambda_):\n",
    "    return compute_loss_mse(y, tx, w) + lambda_ * np.sum(w.T @ w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 Logistic regression\n",
    "## 4.1 Lg using gradient descent\n",
    "### 4.1.1 function implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        gradient = calculate_gradient_logistic(y, tx, w)\n",
    "        w = w - (gamma * gradient)\n",
    "    loss = calculate_loss_logistic(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return (1 / (1 + np.exp(-t)))\n",
    "\n",
    "def fx(x) :\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def calculate_loss_logistic(y, tx, w):\n",
    "    y_predicted = tx @ w\n",
    "    right_hand = y * y_predicted\n",
    "    left_hand = np.apply_along_axis(fx, 0, y_predicted)\n",
    "    return np.sum(left_hand - right_hand)\n",
    "    \n",
    "def calculate_gradient_logistic(y, tx, w):\n",
    "    y_predicted = tx @ w\n",
    "    left_hand = np.apply_along_axis(sigmoid, 0, y_predicted)\n",
    "    return(tx.T @ (left_hand - y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.1 regularized lg using gradient descent \n",
    " ### 4.1.1 function implementation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        gradient = calculate_gradient_reg_logistic(y, tx, lambda_,  w)\n",
    "        w = w - (gamma * gradient)\n",
    "    loss = calculate_loss_reg_logistic(y, tx, lambda_, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return (1 / (1 + np.exp(-t)))\n",
    "\n",
    "def fx(x) :\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def calculate_loss_reg_logistic(y, tx, lambda_,  w):\n",
    "    y_predicted = tx @ w\n",
    "    right_hand = y * y_predicted\n",
    "    left_hand = np.apply_along_axis(fx, 0, y_predicted)\n",
    "    return np.sum(left_hand - right_hand) +((lambda_ / 2.0) * (w.T @ w))\n",
    "    \n",
    "def calculate_gradient_reg_logistic(y, tx, lambda_,  w):\n",
    "    y_predicted = tx @ w\n",
    "    left_hand = np.apply_along_axis(sigmoid, 0, y_predicted)\n",
    "    return(tx.T @ (left_hand - y)) + ( (lambda_ / 2.0) * w )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the data\n",
    "## helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(data, y, w):\n",
    "    y_pred = predict_labels(data=data, weights=w)\n",
    "    error = np.abs(np.sum(y_pred-y))/(2*len(y))*100\n",
    "    print(\"Error: \", error, \"%\")\n",
    "    \n",
    "def predict_labels_changed(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = 0\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def snob_nan(data):\n",
    "    find_col = np.unique(np.argwhere(input_data_te == -999.)[:,1])\n",
    "    return np.delete(data, find_col,axis=1)\n",
    "\n",
    "def corrected_labels(y):\n",
    "    y[np.where(y==-1)] = 0\n",
    "    return y\n",
    "\n",
    "def centralized(data):\n",
    "    #Removing -999\n",
    "    #data[np.where(data==-999.)]= np.nan\n",
    "\n",
    "    #Computing mean and std\n",
    "    mean = np.nanmean(data,axis = 0)\n",
    "    std = np.nanstd(data, axis = 0)\n",
    "\n",
    "    #std the data\n",
    "    return np.nan_to_num((data - mean)/std)\n",
    "\n",
    "def scale(data, precision_factor=1):\n",
    "    min_vec = np.amin(data, axis=0)\n",
    "    max_vec = np.amax(data,axis=0)\n",
    "    interval = precision_factor/np.abs(max_vec-min_vec)\n",
    "    return (data-min_vec)*interval\n",
    "\n",
    "def remove_outliers(data, precision_factor):\n",
    "    std = np.std(data, axis = 0)\n",
    "    mean = np.mean(data,axis = 0)\n",
    "    to_remove = np.argwhere(np.abs(data - mean)/std > precision_factor)  \n",
    "    return np.delete(data, to_remove, axis=0)\n",
    "\n",
    "def shrink_outliers(data, precision_factor):\n",
    "    std = np.std(data, axis = 0)\n",
    "    mean = np.mean(data,axis = 0)\n",
    "    data[np.where((data - mean)/std >   precision_factor)] =   precision_factor*std\n",
    "    data[np.where((data - mean)/std >   precision_factor)] = - precision_factor*std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.432221685\n",
      "(45510, 19)\n",
      "(45510,)\n"
     ]
    }
   ],
   "source": [
    "from split_data import *\n",
    "\n",
    "no_invalid_train = snob_nan(input_data_tr)\n",
    "centralized_train = centralized(no_invalid_train)\n",
    "\n",
    "print(np.max(centralized_train))\n",
    "\n",
    "#nice_data = centralized_train\n",
    "#nice_data = shrink_outliers(centralized_train, 2)\n",
    "nice_data = remove_outliers(centralized_train, precision_factor=3)\n",
    "\n",
    "new_labels = corrected_labels(y_tr)\n",
    "xT, yT, xV, yV = split_data(nice_data, new_labels, 0.8)\n",
    "print(xV.shape)\n",
    "print(yV.shape)\n",
    "\n",
    "\n",
    "\n",
    "#centralized data without outliers nor invalid entries\n",
    "#nice_data = remove_outliers(centralized_train, precision_factor=3)\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with least_squares_GD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  16.03 %\n",
      "None\n",
      "0.147087820959\n"
     ]
    }
   ],
   "source": [
    "loss, w = least_squares_GD(yT, xT, np.zeros(xT.shape[1]), max_iters=100, gamma=0.1)\n",
    "print(test_accuracy(data=xV,w=w, y=yV))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with least_squares_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: (200000,) tx: (200000, 19) w (19,)\n",
      "Error:  17.746 %\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, w = least_squares_SGD(yT, xT, np.zeros(xT.shape[1]), max_iters=100, gamma=0.1)\n",
    "print(test_accuracy(data=xV,w=w, y=yV))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  16.462 %\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.2\n",
    "w, loss = ridge_regression(y=yT, lambda_=lambda_, tx=xT)\n",
    "print(test_accuracy(data=xV,w=w, y=yV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  5.85146121731 %\n",
      "None\n",
      "[ -10.08222802   72.26826456 -110.7764937   158.37629      -5.20497455\n",
      "  -63.84613754   34.91590525  -95.39980423   58.87915856   14.72646811\n",
      "   -9.88807648  -67.88592789   18.3092064     1.53786075  -63.93135875\n",
      "    7.1909712  -123.10212137 -251.53311269  -73.55103216]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "w, loss = logistic_regression(gamma=0.01, initial_w=np.zeros(xT.shape[1]), max_iters=100, tx=xT, y=yT)\n",
    "print(test_accuracy(data=xV,w=w, y=yV))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x,  k, lambda_=0.1, seed=1):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    ws = np.zeros(x.shape[1])\n",
    "    k_indices = build_k_indices(y, k, seed)\n",
    "    for index_k in np.arange(1, k + 1):\n",
    "        \n",
    "        xV = x[k_indices[k - 1]]\n",
    "        yV = y[k_indices[k - 1]]\n",
    "    \n",
    "        xT = np.delete (x, k_indices[k - 1], 0)\n",
    "        yT = np.delete(y, k_indices[k - 1], 0)\n",
    "\n",
    "    \n",
    "        w ,loss = reg_logistic_regression(yT, xT,lambda_, initial_w= np.zeros(input_data_tr.shape[1]) ,gamma=0.2, max_iters=600)\n",
    "        ws = w + ws\n",
    "    ws = ws / k\n",
    "    return ws\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centr_data = scale(centralized(input_data_tr))\n",
    "\n",
    "ws = cross_validation(x=centr_data, y= y_tr, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centr_data_te = centralized(input_data_te)\n",
    "test_accuracy(data=centr_data_te, y=y_te, w=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scale(centralized(input_data_tr),precision_factor=5)\n",
    "print((x==np.nan).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = reg_logistic_regression(lambda_=300, gamma=0.01, initial_w=np.zeros(snob_nan(input_data_tr).shape[1]), tx=snob_nan(input_data_tr),max_iters=200,y=y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracy(data=snob_nan(input_data_tr),w=w, y=y_tr ))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids=ids_te, name=\"Dave\", y_pred=y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
