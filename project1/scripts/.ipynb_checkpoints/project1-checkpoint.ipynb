{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Generate the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "y_tr, input_data_tr, ids_tr = load_csv_data(\"./train.csv\")\n",
    "y_te, input_data_te, ids_te = load_csv_data(\"./test.csv\")\n",
    "input_data_play = input_data_tr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-151bbb2a1641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print min and max of each field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcollumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfiltered_collumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_collumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_collumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "#print min and max of each field\n",
    "for collumn in input_data.T:\n",
    "    filtered_collumn = list(filter(lambda x:x !=-999, collumn))\n",
    "    print(min(filtered_collumn), max(filtered_collumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print distribution of valid values\n",
    "for i in range(30):\n",
    "    print(i)\n",
    "    input_cleaned = list(filter(lambda x : x != -999, input_data.T[i]))\n",
    "\n",
    "    n, bins, patches = plt.hist(input_cleaned, 500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find percentage of most frequent non-zero item\n",
    "from collections import Counter\n",
    "\n",
    "def Most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(1)[0][1]\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    input_cleaned = list(filter(lambda x : x!=-999 and x != 0, input_data.T[i]))\n",
    "    print (Most_Common(input_cleaned)/len(input_data.T[i])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find mapping between fields 22 and 29\n",
    "nuls = input_data[:,[22, 29]]\n",
    "for i in {0,1,2,3}:\n",
    "    input_cleaned = list(map(lambda x:x[1], filter(lambda x : x[0]==i, nuls)))\n",
    "    print(i, \"-> [\",min(input_cleaned), \";\", max(input_cleaned),\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the graph of any given two fields\n",
    "import matplotlib.pyplot as plt\n",
    "tr = input_data.T\n",
    "to_test_1 = 28\n",
    "to_test_2 = 29\n",
    "a,b = zip(*filter(lambda x : x[0]!=-999 and x[1]!=-999, zip(tr[to_test_1], tr[to_test_2])))\n",
    "plt.plot(a, b, 'ro')\n",
    "plt.show()\n",
    "    \n",
    "#compare one field to every others\n",
    "#for i in range(30):\n",
    "#    print(i)\n",
    "#    a,b = zip(*filter(lambda x : x[0]!=-999 and x[1]!=-999, zip(tr[to_test], tr[i])))\n",
    "#    plt.plot(a, b, 'ro')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display correlation matrix\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(data=input_data)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to research if one invalid item is also invalid for other fields\n",
    "inval = input_data[:,[0, 23, 24, 25,4,5,6,12,26,27,28]]\n",
    "are_inval_1 = -999\n",
    "are_inval_2 = -999 * 3\n",
    "are_inval_3 = -999 * 7\n",
    "full_inval_nb = len(list(filter((lambda x: ((x[0]!=are_inval_1) and sum(x[1:4]) ==are_inval_2 and (sum(x[4:])!=are_inval_3))), inval)))\n",
    "print(full_inval_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Least squares methods\n",
    "## 2.1 Least squares  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    w = np.pinv(tx) @ y\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    return w, loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 least squares with gradient descent\n",
    "### 2.2.1 Function implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma) :\n",
    "    # Define parameters to store w and loss\n",
    "    \n",
    "    w = initial_w\n",
    "    loss = -1\n",
    "    for n_iter in range(max_iters):\n",
    "        if (n_iter == max_iters - 1) :\n",
    "            loss = compute_loss_mse(y,tx,w)\n",
    "        \n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        w = w - gamma*grad           \n",
    "        \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 helper funtions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):   \n",
    "    e = y - (tx @ w)\n",
    "    return ((-1/len(y)) * (tx.T @ e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Least squares with stochastic gradient descent\n",
    "### 2.3.1 Function implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma) :\n",
    "    # Define parameters to store w and loss\n",
    "    \n",
    "    w = initial_w\n",
    "    loss = 0\n",
    "    batch_size = 1\n",
    "    \n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, max_iters):\n",
    "        loss = compute_loss_mse(y,tx,w)\n",
    "        grad = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        w = w - gamma*grad\n",
    "\n",
    "           \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    return compute_gradient(y,tx,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of batch_size matching elements from y and tx.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3 Ridge regression\n",
    "## 3.1 function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_ ): \n",
    "    \n",
    "    x_tr = tx.transpose()\n",
    "    lambda_prime = lambda_ * 2 * tx.shape[0]\n",
    "    X = (x_tr @ tx) + (lambda_prime * np.eye(tx.shape[1]))\n",
    "    Y = x_tr @ y\n",
    "    w_ridge = (np.linalg.solve(X, Y))    \n",
    "    loss = compute_loss_ridge(y, tx, w, lambda_)\n",
    "    return w_ridge, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "import numpy as np\n",
    "\n",
    "def compute_loss_ridge (y, tx, w, lambda_):\n",
    "    return compute_loss_mse(y, tx, w) + lambda_ * np.sum(w.T @ w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 Logistic regression\n",
    "## 4.1 Lg using gradient descent\n",
    "### 4.1.1 function implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        gradient = calculate_gradient_logistic(y, tx, w)\n",
    "        w = w - (gamma * gradient)\n",
    "    loss = calculate_loss_logistic(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return (1 / (1 + np.exp(-t)))\n",
    "\n",
    "def fx(x) :\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def calculate_loss_logistic(y, tx, w):\n",
    "    y_predicted = tx @ w\n",
    "    right_hand = y * y_predicted\n",
    "    left_hand = np.apply_along_axis(fx, 0, y_predicted)\n",
    "    return np.sum(left_hand - right_hand)\n",
    "    \n",
    "def calculate_gradient_logistic(y, tx, w):\n",
    "    y_predicted = tx @ w\n",
    "    left_hand = np.apply_along_axis(sigmoid, 0, y_predicted)\n",
    "    return(tx.T @ (left_hand - y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.1 regularized lg using gradient descent \n",
    " ### 4.1.1 function implementation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        gradient = calculate_gradient_reg_logistic(y, tx, lambda_,  w)\n",
    "        w = w - (gamma * gradient)\n",
    "    loss = calculate_loss_reg_logistic(y, tx, lambda_, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return (1 / (1 + np.exp(-t)))\n",
    "\n",
    "def fx(x) :\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def calculate_loss_reg_logistic(y, tx, lambda_,  w):\n",
    "    y_predicted = tx @ w\n",
    "    right_hand = y * y_predicted\n",
    "    left_hand = np.apply_along_axis(fx, 0, y_predicted)\n",
    "    return np.sum(left_hand - right_hand) +((lambda_ / 2.0) * (w.T @ w))\n",
    "    \n",
    "def calculate_gradient_reg_logistic(y, tx, lambda_,  w):\n",
    "    y_predicted = tx @ w\n",
    "    left_hand = np.apply_along_axis(sigmoid, 0, y_predicted)\n",
    "    return(tx.T @ (left_hand - y)) + ( (lambda_ / 2.0) * w )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the data\n",
    "Excluding -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(data, y, w):\n",
    "    y_pred = predict_labels(data=data, weights=w)\n",
    "    error = np.abs(np.sum(y_pred-y))/(2*len(y))*100\n",
    "    print(\"Error: \", error, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized(data):\n",
    "    #Removing -999\n",
    "    data[np.where(data==-999.)]= np.nan\n",
    "\n",
    "    #Computing mean and std\n",
    "    mean = np.nanmean(data,axis = 0)\n",
    "    std = np.nanstd(data, axis = 0)\n",
    "\n",
    "    #std the data\n",
    "    return np.nan_to_num((data - mean)/std)\n",
    "\n",
    "def scale(data, precision_factor=1):\n",
    "    min_vec = np.amin(data, axis=0)\n",
    "    max_vec = np.amax(data,axis=0)\n",
    "    interval = precision_factor/np.abs(max_vec-min_vec)\n",
    "    return (data-min_vec)*interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandredumur/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_scaled = scale(centralized(input_data_tr))\n",
    "\n",
    "wlog, loss = logistic_regression(tx=data_scaled, y=y_tr ,initial_w= np.zeros(input_data_tr.shape[1]) ,gamma=0.2, max_iters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  34.2668 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(data=data_scaled, y = y_tr,w=wlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized(input_data_tr)\n",
    "wreg_log, loss = reg_logistic_regression(y=y_tr, tx=input_data_tr,lambda_=0.1 , initial_w=np.zeros(input_data_tr.shape[1]),max_iters=200, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, x,  k, lambda_=0.1, seed=1):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    ws = np.zeros(x.shape[1])\n",
    "    k_indices = build_k_indices(y, k, seed)\n",
    "    for index_k in np.arange(1, k + 1):\n",
    "        \n",
    "        xV = x[k_indices[k - 1]]\n",
    "        yV = y[k_indices[k - 1]]\n",
    "    \n",
    "        xT = np.delete (x, k_indices[k - 1], 0)\n",
    "        yT = np.delete(y, k_indices[k - 1], 0)\n",
    "\n",
    "    \n",
    "        w ,loss = reg_logistic_regression(yT, xT,lambda_, initial_w= np.zeros(input_data_tr.shape[1]) ,gamma=0.2, max_iters=600)\n",
    "        ws = w + ws\n",
    "    ws = ws / k\n",
    "    return ws\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scale() missing 1 required positional argument: 'precision_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-110a08bae80f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcentr_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentralized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcentr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scale() missing 1 required positional argument: 'precision_factor'"
     ]
    }
   ],
   "source": [
    "centr_data = scale(centralized(input_data_tr))\n",
    "\n",
    "ws = cross_validation(x=centr_data, y= y_tr, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127744\n",
      "85667\n",
      "(250000, 30)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (250000,) (568238,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-44c85b5a1602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentr_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcentr_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-0c4ca36d3c5d>\u001b[0m in \u001b[0;36mtest_accuracy\u001b[0;34m(data, y, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (250000,) (568238,) "
     ]
    }
   ],
   "source": [
    "print(np.sum(predict_labels(data=centr_data, weights=ws)==1))\n",
    "print(np.sum(y_tr==1))\n",
    "print(centr_data.shape)\n",
    "\n",
    "test_accuracy(data=centr_data,y=y_te, w=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  54.0099394972 %\n"
     ]
    }
   ],
   "source": [
    "centr_data_te = centralized(input_data_te)\n",
    "test_accuracy(data=centr_data_te, y=y_te, w=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ..., \n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = scale(centralized(input_data_tr),precision_factor=5)\n",
    "print((x==np.nan).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.15956582e+10,   2.70516690e+10,   2.43163795e+10, ...,\n",
       "         2.63683583e+10,   2.58836218e+10,   2.58176256e+10])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled@w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
